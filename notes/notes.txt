Ch1 What Is Apache Spark?
-------------------------

Spark is a
  unified computing engine
  and a set of libraries
  for parallel data processing
  on computer clusters

Does not store data, but can read data from various sources:
  - cloud (Ayure, S3)
  - HDFS
  - HBase, Cassandra
  - Kafka

Libraries:
  - Spark SQL
  - Spark Streaming / Structured Streaming
  - MLlib
  - GraphX

History:
  - 2009 UC Berkeley
  - 2013- Apache project
  - Databricks
  - 2014: 1.0
  - 2016: 2.0


Ch2 A Gentle Introduction to Spark
----------------------------------

Languages: Scala, Python, Java, R, SQL

Spark Application:
  - 1 driver process + a set of executor processes
  - they run on cluster nodes
  - the nodes / resources are controlled by the cluster manager
  - driver: executes main()
  - executors: get tasks from the driver
  - interactive (spark-shell / pyspark) or standalone (spark-submit)

Cluster Manager:
  - standalone (Spark's built-in)
  - YARN
  - Mesos

1 cluster - multiple Spark applications parallel

Local mode: all in one process (threads), eg: spark-shell

SparkSession:
  - 1 Spark Application - 1 SparkSession
  - 'spark' object in the shell

DataFrame:
  - Stuctured API
  - has a schema
  - a table of data with rows and columns
  - ~ spreadsheet with named columns
  - distributed, split to partitions (handled by Spark)

Transformations:
  - input DataFame -> output DataFame
  - will be executed streamlined when an Action issued
  - narrow dependency/transformation: 1 input partition - 1 output partition
  - wide dependency/transformation: 1 input partition - multiple output partition => shuffle

Actions:
  - trigger computation
  - types:
      - show data in the console
      - collect data to native objects (eg. Java)
      - write data to output data source

Spark UI:
  - port: 4040
  - spark-shell starts it


Ch4 Structured API Overview
---------------------------

- Datasets
- DataFrames
- SQL tables and views (~DataFrame handled through SQL)

The same Structured API for
  - batch
  - streaming
(from Spark 2.2)

DataFrame / Dataset
  table-like collections with well-defined rows and columns

Schema
  defines the column names and types of a DataFrame

Spark Types
  Spark inner data types
  Scala, Python, etc. types are mapped to these common types
  eg. IntegerType, DecimalType (java.math.BigDecimal), StringType, BinaryType (byte[]), TimestampType (java.sql.Timestamp), MapType (java.util.Map)

DataFrame
  "untyped"
  the schema exists only runtime, and there are only runtime checks
  DataFrame ==  Dataset[Row]

Dataset
  "typed"
  compile time schema / types
  Scala case class, Java beans

Row type
  Spark’s internal representation of its optimized in-memory format for computation (instead of JVM types/objects)
  org.apache.spark.sql.Row
  Spark Type: StructType

Columns
  - simple type (eg. integer, string)
  - complex type (eg. arraz, map)
  - null

Rows
  - records of data
  - of type Row


Execution process
  - write code and submit it
  - Logical Plan (generated by Catalyst Optimizer)
  - Physical Plan
  - execution (RDD transformations)


Ch5 Basic Structured Operations
-------------------------------

Schema
  - schema-on-read / schema inference (can be fine for ad hoc analysis)
  - explicit definition (recommended for ETL processes)

Columns
  - ""
  - col()
  - column()
  - df.col()
  - expr()

col("colname") <=> expr("colname")
col("colname") + 5 <=> expr("colname + 5")

Rows
  do not have schema, only DataFrame has schema

DataFrame transformations
  - add row/column
  - remove row/column
  - transform row into column and vice versa
  - change the order of rows


Ch6 Working with Different Types of Data
----------------------------------------

Convert native (eg. Scala) types to Spark types
  lit(5)      Int => IntegerType
  lit("five") String => StringType
  lit(5.0)    Double => DoubleType

Equality in Spark+Scala
  ===   equalTo()   =
  =!=   notEqual()  <> / !=

eqNullSafe()

eq() vs equalTo()

< lt(), etc

And
  should be chained in where().where() (Spark will flatten them)
Or
  only inside where: where(col().or(col()))

Null values:
  - schema does not enforce null check

User defined functions
  - should be written in Scala or Java


Ch09 Data Sources
-----------------

Read modes:
  - permissive (def)
  - dropMalformed
  - failFast

Write modes:
  - append
  - overwrite
  - errorIfExists (def)
  - ignore

Core data source types
  - CSV
  - JSON (typically line-delimited)
  - Parquet
  - ORC
  - JDBC
  - Plain text

Some CSV options
  - sep (,)
  - header (false)
  - inferSchema (false)
  - nullValue ("")
  - dateFormat (yyyy-MM-dd)
  - timestampFormat (yyyy-MM-dd’T’HH:mm​:ss.SSSZZ)
  - compression / codec

Some JSON options
  - dateFormat (yyyy-MM-dd)
  - timestampFormat (yyyy-MM-dd’T’HH:mm​:ss.SSSZZ)
  - compression / codec

Some JDBC options
  - driver
  - url
  - dbtable
  - partitionColumn, lowerBound, upperBound
  - numPartitions
  - fetchsize
  - batchsize

Data loading and partitions
  - 1 file - 1 partition
  - 1 file - multiple partitions
  - multiple files - 1 partition

  - 1 file - 1 executor (read / write)

Bucketing:
  prepartitioning the data (grouping data item that "belong" together)
